import torch
import torch.nn as nn

def train_lr(architecture_G,architecture_D,batch_size:int,n_epochs:int,dataset,lr_g:int,lr_d:int,processing_unit,p_subset:int):

    """
    This function perform the first solution proposed by the paper on how to train the Generator/Discriminator

    ----- INPUT -----
    Generator -> class of the generator
    Discriminator -> class of the discriminator
    batch_size,n_epochs,lr -> number
    dataset -> -> object dataset  

    ----- OUTPUT -----
    metrics to measure the accuracy of the models 

    """

    gen_net = architecture_G
    dis_net = architecture_D
    best_epoch_G = -1
    best_epoch_D = -1

    dataset.preprocessing(p_subset) 
    
    
    optimizer_gen = torch.optim.Adam([p for p in gen_net.net.parameters() if p.requires_grad], lr_g, maximize=True) 
    optimizer_dis = torch.optim.Adam([p for p in dis_net.net.parameters() if p.requires_grad], lr_d, maximize=False)

    best_loss_D = 1000000000000 
    best_loss_G = 1000000000000

    gen_net.net.train()
    dis_net.net.train()

    loss_D_batch = []
    loss_G_batch = []

    for e in range(0,n_epochs):


        last_batch = False

        image_file, _ = dataset.shuffled_dataset()
        length_dataset = len(image_file)

        counter_batches = 0
        avg_loss_D = 0
        avg_loss_G = 0

        while not last_batch:

            counter_batches = counter_batches + 1

            #fake batch creation
            x_gen_input, _ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size)
            current_batch_dim = x_gen_input.shape

            with torch.no_grad():
                #move the data into the processing unit device
                x_gen_input = x_gen_input.to(processing_unit)
                #fake image generated by Generator
                image_output_fake,label_fake_img = gen_net.forward_G(x_gen_input)

            #true image generation
            mini_batch_image_real, mini_batch_label_real, last_batch_flag = dataset.mini_batch_creation(batch_size,image_file)

            #combination real/fake image into the cpu
            image_output_fake = image_output_fake.to('cpu')
            label_fake_img = label_fake_img.to('cpu')
            mini_batch_img_comb, mini_batch_lab_comb  = dis_net.combined_True_Fake(
                fake_labels = label_fake_img, fake_images = image_output_fake, 
                true_labels = mini_batch_label_real , true_images = mini_batch_image_real
            )

            #relocate the combined images into the processing unit 
            mini_batch_img_comb = mini_batch_img_comb.to(processing_unit)
            predicted_label = dis_net.forward_D(mini_batch_img_comb)

            #------------
            # BackProp Discriminator 
            #------------
            
            #move to processing unit 
            predicted_label = predicted_label.to(processing_unit)
            mini_batch_lab_comb = mini_batch_lab_comb.to(processing_unit)

            loss_dis = dis_net.function_loss_D(output_label_dis = predicted_label,true_label = mini_batch_lab_comb)
            loss_D_batch.append(loss_dis.item())

            avg_loss_D = avg_loss_D + loss_dis
            print("loss DISCRIMINATOR,", loss_dis ,avg_loss_D)

            optimizer_dis.zero_grad()
            loss_dis.backward()
            optimizer_dis.step()


            #----------
            #BackProp Generator
            #----------

            x_gen_input,_ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size, current_batch_dim = current_batch_dim)
            x_gen_input = x_gen_input.to(processing_unit)

            image_output_fake,fake_target = gen_net.forward_G(x_gen_input)
            
            predicted_label_generated = dis_net.forward_D(image_output_fake)
            fake_target = fake_target.view(-1,1)

            #move to processing unit
            predicted_label_generated = predicted_label_generated.to(processing_unit)
            fake_target = fake_target.to(processing_unit)

            loss_gen = gen_net.function_loss_G(output_label_gen = predicted_label_generated, true_label = fake_target)

            loss_G_batch.append(loss_gen.item())
            avg_loss_G = avg_loss_G + loss_gen 

            print("loss GENERATOR", loss_gen, avg_loss_G)

            optimizer_gen.zero_grad()
            loss_gen.backward()
            optimizer_gen.step()
            

            last_batch = last_batch_flag


        #gen_net.image_generation()

        avg_loss_G = avg_loss_G/counter_batches
        avg_loss_D = avg_loss_D/counter_batches

        if avg_loss_D < best_loss_D:
            best_loss_D = avg_loss_D
            best_epoch_D = e + 1
            dis_net.save("weights_discriminator.pth")
            
        if avg_loss_G < best_loss_G:
          best_loss_G = avg_loss_G
          best_epoch_G = e + 1
          gen_net.save("weights_generator.pth")

       
        print("epoch:{0}/{1}".format(e+1,n_epochs)) 
              
        print(("DISCRIMIATOR: BCE LOSS={0:.4f}"
                   + (", BEST!" if avg_loss_D == best_loss_D else ""))
                  .format(avg_loss_D))
            
        print(("GENERATOR: BCE LOSS={0:.4f}"
                   + (", BEST!" if avg_loss_G == best_loss_G else ""))
                  .format(avg_loss_G))
            
        print("========================================================================================= \n")



def train_lr_obj(architecture_G,architecture_D,batch_size:int,n_epochs:int,dataset,lr_g:int,lr_d:int,processing_unit,p_subset:int):

    """
    Here I modify the learning rate and also the cost function of the Generator. Indeed, now I do not maximize sum(-(1-y)log(D(G))) but rather
    I aim to minizime sum(ylog(D(G))). For this reason i falsly impose y = 1. 

    ----- INPUT -----
    Generator -> class of the generator
    Discriminator -> class of the discriminator
    batch_size,n_epochs,lr -> number
    dataset -> -> object dataset  

    ----- OUTPUT -----
    metrics to measure the accuracy of the models 

    """

    gen_net = architecture_G
    dis_net = architecture_D
    best_epoch_G = -1
    best_epoch_D = -1

    dataset.preprocessing(p_subset) 
    

    
    optimizer_gen = torch.optim.Adam([p for p in gen_net.net.parameters() if p.requires_grad], lr_g, maximize=False) 
    optimizer_dis = torch.optim.Adam([p for p in dis_net.net.parameters() if p.requires_grad], lr_d, maximize=False)

    best_loss_D = 1000000000000 
    best_loss_G = 1000000000000

    gen_net.net.train()
    dis_net.net.train()

    loss_G_batch = []
    loss_D_batch = []

    for e in range(0,n_epochs):


        last_batch = False

        image_file, _ = dataset.shuffled_dataset()
        length_dataset = len(image_file)

        counter_batches = 0
        avg_loss_D = 0
        avg_loss_G = 0

        while not last_batch:

            counter_batches = counter_batches + 1

            #fake batch creation
            x_gen_input, _ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size)
            current_batch_dim = x_gen_input.shape

            with torch.no_grad():
                #move the data into the processing unit device
                x_gen_input = x_gen_input.to(processing_unit)
                #fake image generated by Generator
                image_output_fake,label_fake_img = gen_net.forward_G(x_gen_input)

            #true image generation
            mini_batch_image_real, mini_batch_label_real, last_batch_flag = dataset.mini_batch_creation(batch_size,image_file)

            #combination real/fake image into the cpu
            image_output_fake = image_output_fake.to('cpu')
            label_fake_img = label_fake_img.to('cpu')
            mini_batch_img_comb, mini_batch_lab_comb  = dis_net.combined_True_Fake(
                fake_labels = label_fake_img, fake_images = image_output_fake, 
                true_labels = mini_batch_label_real , true_images = mini_batch_image_real
            )

            #relocate the combined images into the processing unit 
            mini_batch_img_comb = mini_batch_img_comb.to(processing_unit)
            predicted_label = dis_net.forward_D(mini_batch_img_comb)

            #------------
            # BackProp Discriminator 
            #------------
            
            #move to processing unit 
            predicted_label = predicted_label.to(processing_unit)
            mini_batch_lab_comb = mini_batch_lab_comb.to(processing_unit)

            loss_dis = dis_net.function_loss_D(output_label_dis = predicted_label,true_label = mini_batch_lab_comb)

            loss_D_batch.append(loss_dis.item())

            avg_loss_D = avg_loss_D + loss_dis
            print("loss DISCRIMINATOR,", loss_dis ,avg_loss_D)

            optimizer_dis.zero_grad()
            loss_dis.backward()
            optimizer_dis.step()


            #----------
            #BackProp Generator
            #----------

            x_gen_input,_ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size, current_batch_dim = current_batch_dim)
            x_gen_input = x_gen_input.to(processing_unit)

            image_output_fake,_ = gen_net.forward_G(x_gen_input)

            predicted_label_generated = dis_net.forward_D(image_output_fake)

            #move to processing unit
            predicted_label_generated = predicted_label_generated.to(processing_unit)
            mini_batch_label_real = mini_batch_label_real.view(-1,1).to(processing_unit)#mini_batch_label_real is a tensor full of 1s

            loss_gen = gen_net.function_loss_G(output_label_gen=predicted_label_generated, true_label=mini_batch_label_real)    

            loss_G_batch.append(loss_gen.item())
            avg_loss_G = avg_loss_G + loss_gen 

            print("loss GENERATOR", loss_gen, avg_loss_G)

            optimizer_gen.zero_grad()
            loss_gen.backward()
            optimizer_gen.step()
            

            last_batch = last_batch_flag



        avg_loss_G = avg_loss_G/counter_batches
        avg_loss_D = avg_loss_D/counter_batches

        if avg_loss_D < best_loss_D:
            best_loss_D = avg_loss_D
            best_epoch_D = e + 1
            dis_net.save("weights_discriminator.pth")
            
        if avg_loss_G < best_loss_G:
          best_loss_G = avg_loss_G
          best_epoch_G = e + 1
          gen_net.save("weights_generator.pth")

       
        print("epoch:{0}/{1}".format(e+1,n_epochs)) 
              
        print(("DISCRIMIATOR: BCE LOSS={0:.4f}"
                   + (", BEST!" if avg_loss_D == best_loss_D else ""))
                  .format(avg_loss_D))
            
        print(("GENERATOR: BCE LOSS={0:.4f}"
                   + (", BEST!" if avg_loss_G == best_loss_G else ""))
                  .format(avg_loss_G))
            
        print("========================================================================================= \n")

