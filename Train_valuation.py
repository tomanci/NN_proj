
def train(Generator,Discriminator,batch_size,n_epochs,dataset,lr):

    """
    ----- INPUT -----
    Generator -> class of the generator
    Discriminator -> class of the discriminator
    batch_size,n_epochs,lr -> number
    dataset -> -> object dataset  

    ----- OUTPUT -----
    metrics to measure the accuracy of the models 

    """

    gen_net = Generator
    dis_net = Discriminator
    best_epoch_G = -1
    best_epoch_D = -1

    dataset.preprocessing() #potrebbe dare problemi qui e nei metodi di dataset() 
    

    lr = lr

    optimizer_gen = torch.optim.Adam([p for p in gen_net.parameters() if p.requires_grad], lr) #torch.optim.Adam(filter(lambda p: p.requires_grad, self.net.parameters()), lr)
    optimizer_dis = torch.optim.Adam([p for p in dis_net.parameters() if p.requires_grad], lr)

    lowest_loss_gen = 1000000000000 
    lowest_loss_dis = 1000000000000

    for e in range(0,n_epochs):

        gen_net.net.train()
        dis_net.net.train()

        last_batch = False

        image_file, _ = dataset.shuffled_dataset()
        length_dataset = len(image_file)

        counter_batches = 0
        tot_loss_D = 0
        tot_loss_G = 0

        while not last_batch:

            counter_batches = counter_batches + 1

            #fake batch creation
            x_gen_input, _ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size)

            #fake image generated by Generator
            image_output_fake,label_fake_img = gen_net.forward_G(x_gen_input)

            #true image generation
            mini_batch_image_real, mini_batch_label_real, last_batch_flag = dataset.mini_batch_creation(batch_size,image_file)            

            #combination real/fake image
            mini_batch_img_comb, mini_batch_lab_comb  = dis_net.combined_True_Fake(
                fake_labels = label_fake_img, fake_images = image_output_fake, 
                true_labels = mini_batch_label_real , true_images = mini_batch_image_real
            )

            predicted_label = dis_net.forward_D(mini_batch_img_comb)

            #------------
            # BackProp Discriminator 
            #------------

            loss_dis = dis_net.function_loss_D(output_label_dis = predicted_label,true_label = mini_batch_lab_comb)
            tot_loss_D = tot_loss_D + loss_dis

            optimizer_dis.zero_grad()
            loss_dis.backward()
            optimizer_dis.step()


            #----------
            #BackProp Generator
            #----------

            predicted_fake_label = separation(predicted_label)

            loss_gen = gen_net.function_loss_G(output_label_gen = predicted_fake_label, true_label = label_fake_img)
            tot_loss_G = tot_loss_G - loss_gen #remember, loss_gen is negative becaudse we want to maximize

            optimizer_gen.zero_grad()
            loss_gen.backward()
            optimizer_gen.step()
            

            last_batch = last_batch_flag

        with torch.no_grad():

            avg_loss_G = tot_loss_G/counter_batches
            avg_loss_D = tot_loss_D/counter_batches

            gen_net.image_generation()

            if avg_loss_D < lowest_loss_dis:
                lowest_loss_dis = loss_dis
                best_epoch_D = e + 1
                dis_net.save("weights_discriminator.pth")
            

            if avg_loss_G < lowest_loss_gen:
                lowest_loss_gen = loss_gen
                best_epoch_G = e + 1
                gen_net.save("weights_generator.pth")

            print("epoch:{0}/{1}".format(e+1,n_epochs))
            print("BEST weights paremeters for GENERATOR: {0} " if lowest_loss_gen == avg_loss_G else " Error committed by GENERATOR: {0}".format(avg_loss_G,best_epoch_G))
            print("BEST weights paremeters for DISCRIMINATOR: {0} " if lowest_loss_dis == avg_loss_D else " Error committed by DISCRIMINATOR: {0}".format(avg_loss_D,best_epoch_D))
            print("========================================================================================= \n")

    
    gen_net.image_generation()



            





def separation(predicted_label):

    """
    divide the true from the generated data in orded to do backprop in the generator 
    """

    length = int(predicted_label.shape[0]/2)

    fake_label = predicted_label[length:]
    fake_label = fake_label.view(1,-1)

    return fake_label

def save(self, file_name):
    """Save the architecture."""
    torch.save(self.net.state_dict(), file_name)
    