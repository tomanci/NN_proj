{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8kcBpsXh9ikN"
      },
      "outputs": [],
      "source": [
        "#package required\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil \n",
        "import PIL\n",
        "import os \n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import transforms \n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBv2cPtN-Gr6"
      },
      "source": [
        "# class dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-UJZNGmK9rU5"
      },
      "outputs": [],
      "source": [
        "class Dataset():\n",
        "    \"\"\"\n",
        "    We transfer the kaggle dataset into a folder where the python environment is. \n",
        "    path_source is where the kaggle dataset downloaded is, path destination is where it should be placed the unzipped dataset.[by default there are my paths] \n",
        "    \"\"\"\n",
        "    def __init__(self,path_destination):\n",
        "\n",
        "        self.path_destination = path_destination\n",
        "        self.file_id = 0\n",
        "        self.files = []\n",
        "        self.true_labels = []\n",
        "        \n",
        "\n",
        "        !pip install unzip\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/gdrive')\n",
        "        !cp /content/gdrive/MyDrive/NN_dataset/img_celeba_dataset.zip /content\n",
        "        !unzip \"/content/img_celeba_dataset.zip\" -d \"/content\"\n",
        "               \n",
        "        \n",
        "    def preprocessing(self,p_subset=0.25):\n",
        "        \n",
        "        \"\"\"\n",
        "        preprocessing function creates the dataset folder where a subset of the original images is taken \n",
        "        --- INPUT ---\n",
        "        p_subset = the percentage of the image transfered to the dataset default = 0.25\n",
        "\n",
        "        --- OUTPUT ---\n",
        "        dataset folder with the preprocessed images. \n",
        "        Each image has undergone to the following processes\n",
        "        1) crop it and centralized \n",
        "        2) downsampled to 64x64 resolution\n",
        "        3) converted from a RGB to a grayscale image [computational reason]\n",
        "        \"\"\"\n",
        "\n",
        "        self.p_subset = p_subset\n",
        "\n",
        "        #creation of the path where the new dataset will be stored\n",
        "        self.path_dataset = os.path.join(os.getcwd(),\"dataset\")\n",
        "        os.mkdir(self.path_dataset)\n",
        "\n",
        "        self.counter_photos = 0\n",
        "        self.counter_dataset = 0\n",
        "\n",
        "        for photo in os.listdir(self.path_destination):\n",
        "\n",
        "            self.counter_photos = self.counter_photos +1\n",
        "            if torch.rand(1) <= p_subset:\n",
        "\n",
        "                self.counter_dataset = self.counter_dataset +1\n",
        "                #directory of each photo example (../img_celeba_dataset/19736.jpeg)\n",
        "                dir = os.path.join(self.path_destination,photo)\n",
        "\n",
        "                #in sequence, I open the photo, crop it, resize(downsampled to a 64x64 resolution)   \n",
        "                im = Image.open(dir).crop((20,45,150,185)).resize((64,64))#140x140 \n",
        "                #save the new image into that directory\n",
        "                im.save(str(self.path_dataset+\"/\"+\"real_\"+photo))\n",
        "\n",
        "        print(\"counter photos {} counter dataset {} ratio {}\".format(self.counter_photos,self.counter_dataset, self.counter_dataset/self.counter_photos))\n",
        "\n",
        "    \n",
        "    def shuffled_dataset(self,boolean_shuffle=True):\n",
        "\n",
        "        \"\"\"\n",
        "        mixed the dataset, by returning a list containing the path of every single image, which will be needed when we have to upload the mini batch\n",
        "        a trivial example of an entries of self.files is like: \"/Users/tommasoancilli/Desktop/Python/NN_proj/dataset/real_8328239.jpg\"\n",
        "        \"\"\"\n",
        "        self.path_dataset = os.path.join(os.getcwd(),\"dataset/\")\n",
        "\n",
        "        self.files = [os.path.join(self.path_dataset,f) for f in os.listdir(self.path_dataset)]\n",
        "        self.true_labels = [1]*len(self.files)\n",
        "\n",
        "        if boolean_shuffle:\n",
        "            shuffled_list = torch.randperm(len(self.files))\n",
        "\n",
        "            self.files = [self.files[i] for i in shuffled_list]\n",
        "            self.true_labels = [self.true_labels[i] for i in shuffled_list]\n",
        "\n",
        "        return self.files, self.true_labels\n",
        "\n",
        "\n",
        "    def load_and_conversion(self,file_img):\n",
        "\n",
        "        \"\"\"\n",
        "        it loads and converts the images into a tensor format \n",
        "        ----- INPUT -----\n",
        "\n",
        "        ----- OUTPUT -----\n",
        "\n",
        "        img_tensor : tensor of the image, being converted from a .jpg format to a tensor one\n",
        "        label_tensor : tensor of the label \n",
        "        end_dataset : flag showing the bottom of the dataset \n",
        "        \"\"\"\n",
        "        self.files = file_img\n",
        "        self.end_dataset = False\n",
        "\n",
        "        im = Image.open(self.files[self.file_id])\n",
        "        label = self.true_labels[self.file_id]\n",
        "         \n",
        "        convert_tensor = transforms.ToTensor()\n",
        "\n",
        "        img_tensor = convert_tensor(im)\n",
        "        label_tensor = torch.tensor(label)\n",
        "\n",
        "        if self.file_id < len(self.files)-1:\n",
        "            self.file_id = self.file_id +1\n",
        "        else:\n",
        "            self.end_dataset = True\n",
        "            self.file_id = 0 #reset the index\n",
        "\n",
        "\n",
        "        return img_tensor,label_tensor,self.end_dataset\n",
        "\n",
        "\n",
        "\n",
        "    def mini_batch_creation(self, batch_size:int,file_img):\n",
        "        \"\"\"\n",
        "        create the a batch containing the images processed\n",
        "        ------ INPUT ------\n",
        "        batch_size: given by the users \n",
        "\n",
        "        ----- OUTPUT -----\n",
        "        mini_batch_image: batch of images, its shape is a 4D tensor #batch_size x #channels x width x height\n",
        "        mini_batch_labels: batch of labels, its a tensor: 1 x #batch_size \n",
        "        last_batch_flag: flag variable to say when we have reached the last batch. In this way, in the training function it will indicate the end of a epoch \n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        i = 0\n",
        "        self.data_batch_im = []\n",
        "        self.data_batch_label = []\n",
        "\n",
        "        last_batch_flag = False\n",
        "\n",
        "        while i < batch_size:\n",
        "            \n",
        "            img,label,end_dataset_flag = self.load_and_conversion(file_img)\n",
        "\n",
        "            self.data_batch_im.append(img)\n",
        "            self.data_batch_label.append(label)\n",
        "            last_batch = end_dataset_flag\n",
        "\n",
        "            i = i + 1\n",
        "\n",
        "            if last_batch:\n",
        "                last_batch_flag = True\n",
        "                break\n",
        "        \n",
        "        mini_batch_image = torch.stack(self.data_batch_im,dim=0)\n",
        "        mini_batch_label = torch.stack(self.data_batch_label,dim=0).view(1,-1)\n",
        "\n",
        "        return mini_batch_image, mini_batch_label, last_batch_flag\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWR8ZEEB-yHw"
      },
      "source": [
        "# ANN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R2Go0_8Z-2Zx"
      },
      "outputs": [],
      "source": [
        "class Generator(): #puÃ² sia essere il Generator nelle GAN che una classica NN negli adversarial attack\n",
        "\n",
        "    \"\"\"\n",
        "    input: tensor (5,5,5) x,y,z dimension\n",
        "\n",
        "    output: tenror (64,64,3) xyz\n",
        "\n",
        "    Inner structure: with the parameter \"structure\" an autoencoder net or a paper-based net can be deployed   \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,processing_unit,structure):\n",
        "        \n",
        "        if structure == \"CNN\":\n",
        "          self.layers = []\n",
        "\n",
        "          self.layers.append(nn.Conv2d(3, 64, kernel_size=3, padding=1)) #stride = 1 default\n",
        "          self.layers.append(nn.LeakyReLU(negative_slope=0.01, inplace=True))#layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "          self.layers.append(nn.Conv2d(64, 128, kernel_size=5,padding=2)) #stride = 1 default\n",
        "          self.layers.append(nn.LeakyReLU(negative_slope=0.01, inplace=True))\n",
        "\n",
        "          #depthwise separable CNN layer\n",
        "          #self.layers.append(nn.Conv2d(128,128,kernel_size=7,padding=3,groups=128))\n",
        "          #self.layers.append(nn.Conv2d(128,256,kernel_size=1))\n",
        "          self.layers.append(nn.Conv2d(128, 256, kernel_size=7, padding=3)) #stride = 1 default\n",
        "          self.layers.append(nn.LeakyReLU(negative_slope=0.01, inplace=True))\n",
        "\n",
        "          self.layers.append(nn.Conv2d(256, 64, kernel_size=3, padding=1)) #stride = 1 default\n",
        "          self.layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "          self.layers.append(nn.Conv2d(64, 3, kernel_size=3, padding=1)) #stride = 1 default\n",
        "          self.layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "          self.net = nn.Sequential(*self.layers)\n",
        "\n",
        "          self.input_shape = (3,64,64)\n",
        "\n",
        "        \n",
        "        elif structure == \"conventional\":\n",
        "        \n",
        "          self.layers = []\n",
        "\n",
        "          self.layers.append(nn.ConvTranspose2d(in_channels=5,out_channels=128,kernel_size=4,padding=2,stride=1))#[5,5,5] -> [4,4,128]\n",
        "          self.layers.append(nn.ReLU())\n",
        "          self.layers.append(nn.BatchNorm2d(128))\n",
        "\n",
        "          self.layers.append(nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=3,padding=2,stride=3))#[4,4,128] -> [8,8,64]\n",
        "          self.layers.append(nn.ReLU())\n",
        "          self.layers.append(nn.BatchNorm2d(64))\n",
        "\n",
        "          self.layers.append(nn.ConvTranspose2d(in_channels=64,out_channels=32,kernel_size=4,padding=1,stride=2))#[8,8,64] -> [16,16,32]\n",
        "          self.layers.append(nn.ReLU())\n",
        "          self.layers.append(nn.BatchNorm2d(32))\n",
        "\n",
        "          self.layers.append(nn.ConvTranspose2d(in_channels=32,out_channels=16,kernel_size=4,padding=1,stride=2))#[16,16,32] -> [32,32,16]\n",
        "          self.layers.append(nn.ReLU())\n",
        "\n",
        "          self.layers.append(nn.ConvTranspose2d(in_channels=16,out_channels=3,kernel_size=2,padding=0,stride=2))#[32,32,16] -> [64,64,3]\n",
        "          self.layers.append(nn.ReLU())\n",
        "\n",
        "          self.net = nn.Sequential(*self.layers)\n",
        "\n",
        "          self.input_shape = (5,5,5)\n",
        "\n",
        "        \n",
        "\n",
        "        self.device = torch.device(processing_unit)\n",
        "        self.net = self.net.to(self.device)\n",
        "        self.position = 0\n",
        "\n",
        "\n",
        "    \n",
        "    def forward_G(self, x_input:torch.tensor):\n",
        "\n",
        "      \"\"\"\n",
        "      ----- Input -----\n",
        "      x_input -> tensor, dimension #example_mini_batch x #channels x width x height \n",
        "\n",
        "      ----- Output ----\n",
        "      self.image_output -> tensor, dimension #example_mini_batch #channels x width x height \n",
        "      self.label_fake_img -> tensor, dimension 1 x #example_mini_batch\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      self.image_output = self.net(x_input)\n",
        "      self.label_fake_img = torch.tensor([[0]*len(x_input)])\n",
        "\n",
        "      return self.image_output,self.label_fake_img\n",
        "\n",
        "\n",
        "    def function_loss_G(self,output_label_gen:torch.tensor,true_label:torch.tensor):\n",
        "      \"\"\"\n",
        "      ----- INPUT -----\n",
        "      output_label_gen -> tensor 1 x #batch_sample\n",
        "      true_label -> tensor 1 x #batch_sample\n",
        "      ----- OUTPUT -----\n",
        "\n",
        "      loss function. \n",
        "      (it is negative because we want to maximize and generally backprop follows the discent of the gradient, so the max = - min )\n",
        "      \"\"\"\n",
        "      cost_function = nn.BCELoss()\n",
        "      true_label = true_label.to(torch.float) #aggiunto questo \n",
        "      loss = cost_function(output_label_gen,true_label)#-cost_function(output_label_gen,true_label)\n",
        "\n",
        "      return loss\n",
        "\n",
        "\n",
        "    def image_generation(self):\n",
        "      \"\"\"\n",
        "      This function will be used to display a image after the ending of batch/epoch...\n",
        "      \"\"\"\n",
        "\n",
        "      #creation of a tuple (1,self.input_shape)\n",
        "      self.net.load_state_dict(torch.load(\"weights_generator.pth\", map_location=self.device))\n",
        "      random_input = torch.rand(size= (1,)+self.input_shape)\n",
        "      random_input = random_input.to(self.device)\n",
        "      transform = T.ToPILImage()#function to transform a tensor into a image\n",
        "      out = self.net(random_input)\n",
        "      #since our input is a 4D tensor, to create a RGB image we have to change their view\n",
        "      out = out.view(out.shape[0]*out.shape[1],out.shape[2],out.shape[3])\n",
        "      im = transform(out)\n",
        "\n",
        "      return im#im.show()\n",
        "      \n",
        "    def creation_image(self, length_dataset:int):\n",
        "\n",
        "      \"\"\"\n",
        "        create a single (4D) tensor image\n",
        "\n",
        "        ----- INPUT -----\n",
        "        length_dataset -> int value\n",
        "\n",
        "        ----- OUTPUT ------\n",
        "        a tensor image (img) and a boolean function if the we have reached the end of the dataset\n",
        "      \"\"\"\n",
        "\n",
        "      img = torch.rand(self.input_shape)\n",
        "\n",
        "      end_dataset = False\n",
        "      \n",
        "      if self.position <  length_dataset -1:\n",
        "        self.position = self.position +1 \n",
        "      else:\n",
        "        self.position = 0\n",
        "        end_dataset = True\n",
        "\n",
        "      return img, end_dataset\n",
        "    \n",
        "    def input_creation(self,length_dataset:int,batch_size:int,current_batch_dim = None):\n",
        "      \"\"\"\n",
        "      This function create a tensor for feeding the generator net. dimension (batch_size x channels x width x height)\n",
        "      If current_batch_dim = None, the output is feed into the discriminator without propagating the gradient till the generator. The presence of this parameter indicates whether the discrimiator is trained or not.\n",
        "\n",
        "      ----- INPUT -----\n",
        "      length_dataset -> int value \n",
        "      batch_size -> int value\n",
        "\n",
        "      ----- OUTPUT -----\n",
        "      batch_input, 4D tensor: #sample_batch x #channels x width x height  \n",
        "      \"\"\"\n",
        "      if current_batch_dim == None:\n",
        "\n",
        "        length_dataset = length_dataset\n",
        "        i = 0\n",
        "        self.batch_input = []\n",
        "\n",
        "        last_batch = False\n",
        "\n",
        "        while i < batch_size:\n",
        "\n",
        "          img,end_dataset_flag = self.creation_image(length_dataset)\n",
        "          self.batch_input.append(img)\n",
        "          last_batch = end_dataset_flag\n",
        "\n",
        "          i = i + 1\n",
        "        \n",
        "          if last_batch:\n",
        "            break\n",
        "\n",
        "        self.batch_input = torch.stack(self.batch_input,dim = 0)\n",
        "\n",
        "      else:\n",
        "        \n",
        "        last_batch = False\n",
        "        \n",
        "        self.batch_input = torch.rand((current_batch_dim))\n",
        "\n",
        "\n",
        "      return self.batch_input, last_batch\n",
        "\n",
        "\n",
        "    def summary(self):\n",
        "      \"\"\"\n",
        "      summary of the model \n",
        "      \"\"\"\n",
        "      for idx,l in enumerate(self.layers):\n",
        "        print(idx+1, \"->\",l)\n",
        "        if \"LU\" in str(l):\n",
        "          print(\"\\n\")\n",
        "    \n",
        "    def save(self,file_name):\n",
        "      torch.save(self.net.state_dict(), file_name)\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator():\n",
        "\n",
        "    def __init__(self,processing_unit):\n",
        "        self.layers = []\n",
        "\n",
        "        self.layers.append(nn.Conv2d(3,32,kernel_size=7,padding = \"same\"))#64x64x32\n",
        "        self.layers.append(nn.AvgPool2d(2,stride=2))#32x32x32\n",
        "        self.layers.append(nn.LeakyReLU(inplace=True))\n",
        "\n",
        "        self.layers.append(nn.Conv2d(32,64,kernel_size=5,padding=\"same\"))#32x32x64\n",
        "        self.layers.append(nn.AvgPool2d(2,stride=2))#16x16x64\n",
        "        self.layers.append(nn.LeakyReLU(inplace=True))\n",
        "\n",
        "        self.layers.append(nn.Conv2d(64,128,kernel_size=3,padding=1,stride=2))#8x8x128\n",
        "        self.layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.layers.append(nn.Conv2d(128,256,kernel_size=3,padding=1,stride=2))#4x4x256\n",
        "        self.layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.layers.append(nn.Flatten())#256X16\n",
        "\n",
        "        self.layers.append(nn.Linear(in_features=256*4*4,out_features=1))\n",
        "\n",
        "        self.layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.net = nn.Sequential(*self.layers)\n",
        "\n",
        "        self.device = torch.device(processing_unit)\n",
        "        self.net = self.net.to(self.device)\n",
        "\n",
        "    \n",
        "    def combined_True_Fake(self, fake_labels:torch.tensor, fake_images:torch.tensor, true_labels:torch.tensor, true_images:torch.tensor):\n",
        "      \"\"\"\n",
        "      The true and generated images are combined together \n",
        "        ----- Input ----\n",
        "        fake_labels, fake_images, true_labels, true_images -> tensor\n",
        "        dimensions:\n",
        "        fake_images, true images = #of_example_mini_batch x #channles x width x height\n",
        "        fake/true labels = 1x#of_example_mini_batch\n",
        "\n",
        "        ----- Output ------\n",
        "        combined_images, combined_label -> tensor\n",
        "        dimension:\n",
        "        combined_label = 1 x #examples_mini_batch\n",
        "        combined_images = 2*#examples_mini_batch x width x height \n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "      combined_images = torch.stack([true_images,fake_images],dim=0)\n",
        "      # combined_images dimension = 2(number of tensor combined into a list) x #mini_batches x channels x width x height\n",
        "      combined_images = combined_images.view(\n",
        "        combined_images.shape[0]*combined_images.shape[1],\n",
        "        combined_images.shape[2],combined_images.shape[3],combined_images.shape[4]\n",
        "      )\n",
        "      #with view I reshape the dimension, compressing the first two dimention into a single one.\n",
        "      #it's like pilling up the tensor one after the other in the dimension of the mini batches \n",
        "\n",
        "      #hstack -> it stacks the tensor horizionally, so from two tensor of shape [1,n] and [1,n] it prints out [1,2*n]\n",
        "      combined_labels = torch.hstack([true_labels,fake_labels])\n",
        "      combined_labels = combined_labels.view(-1,1)\n",
        "      #view(-1,1) it's like numpy.reshape and it \"transpose\" creating a [2*n,1] vector \n",
        "\n",
        "      return combined_images,combined_labels\n",
        "\n",
        "    def forward_D(self,images:torch.tensor):\n",
        "      \"\"\"\n",
        "      ----- INPUT -----\n",
        "      image -> tensor 2*#batch_size x #channels x width x height\n",
        "\n",
        "      ----- OUTPUT -----\n",
        "      tensor -> 2*#batch_size x 1\n",
        "      \"\"\"\n",
        "      self.output = self.net(images)\n",
        "      self.output = self.output.to(torch.float32) #aggiunto questo \n",
        "      return self.output \n",
        "      \n",
        "    def function_loss_D(self,output_label_dis:torch.tensor,true_label:torch.tensor):\n",
        "      \"\"\"\n",
        "      ----- INPUT -----\n",
        "      output_label_dis -> 2*#batch_size x 1\n",
        "      true_label -> 2*#batch_size x 1 \n",
        "      \n",
        "      ----- OUTPUT -----\n",
        "      loss function\n",
        "\n",
        "      \"\"\"\n",
        "      cost_function = nn.BCELoss()\n",
        "      true_label = true_label.to(torch.float) #aggiunto questo \n",
        "      loss = cost_function(output_label_dis,true_label)\n",
        "\n",
        "      return loss\n",
        "\n",
        "    def save(self,file_name):\n",
        "      torch.save(self.net.state_dict(), file_name)\n",
        "\n",
        "    def summary(self):\n",
        "      \"\"\"\n",
        "      summary of the model \n",
        "      \"\"\"\n",
        "      for idx,l in enumerate(self.layers):\n",
        "        print(idx+1, \"->\",l)\n",
        "        if \"LU\" in str(l):\n",
        "          print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_22igvDX_Deu"
      },
      "source": [
        "# Train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1PXaV6NG_T_w"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_lr(architecture_G,architecture_D,batch_size:int,n_epochs:int,dataset,lr_g:int,lr_d:int,processing_unit,p_subset:int):\n",
        "\n",
        "    \"\"\"\n",
        "    This function perform the first solution proposed by the paper on how to train the Generator/Discriminator\n",
        "\n",
        "    ----- INPUT -----\n",
        "    Generator -> class of the generator\n",
        "    Discriminator -> class of the discriminator\n",
        "    batch_size,n_epochs,lr -> number\n",
        "    dataset -> -> object dataset  \n",
        "\n",
        "    ----- OUTPUT -----\n",
        "    metrics to measure the accuracy of the models \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    gen_net = architecture_G\n",
        "    dis_net = architecture_D\n",
        "    best_epoch_G = -1\n",
        "    best_epoch_D = -1\n",
        "\n",
        "    dataset.preprocessing(p_subset) \n",
        "    \n",
        "    \n",
        "    optimizer_gen = torch.optim.Adam([p for p in gen_net.net.parameters() if p.requires_grad], lr_g, maximize=True) \n",
        "    optimizer_dis = torch.optim.Adam([p for p in dis_net.net.parameters() if p.requires_grad], lr_d, maximize=False)\n",
        "\n",
        "    best_loss_D = 1000000000000 \n",
        "    best_loss_G = 1000000000000\n",
        "\n",
        "    gen_net.net.train()\n",
        "    dis_net.net.train()\n",
        "\n",
        "    loss_G_batch = []\n",
        "    loss_D_batch = []\n",
        "\n",
        "\n",
        "    for e in range(0,n_epochs):\n",
        "\n",
        "\n",
        "        last_batch = False\n",
        "\n",
        "        image_file, _ = dataset.shuffled_dataset()\n",
        "        length_dataset = len(image_file)\n",
        "\n",
        "        counter_batches = 0\n",
        "        avg_loss_D = 0\n",
        "        avg_loss_G = 0\n",
        "\n",
        "        while not last_batch:\n",
        "\n",
        "            counter_batches = counter_batches + 1\n",
        "\n",
        "            #fake batch creation\n",
        "            x_gen_input, _ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size)\n",
        "            current_batch_dim = x_gen_input.shape\n",
        "\n",
        "            with torch.no_grad():\n",
        "                #move the data into the processing unit device\n",
        "                x_gen_input = x_gen_input.to(processing_unit)\n",
        "                #fake image generated by Generator\n",
        "                image_output_fake,label_fake_img = gen_net.forward_G(x_gen_input)\n",
        "\n",
        "            #true image generation\n",
        "            mini_batch_image_real, mini_batch_label_real, last_batch_flag = dataset.mini_batch_creation(batch_size,image_file)\n",
        "\n",
        "            #combination real/fake image into the cpu\n",
        "            image_output_fake = image_output_fake.to('cpu')\n",
        "            label_fake_img = label_fake_img.to('cpu')\n",
        "            mini_batch_img_comb, mini_batch_lab_comb  = dis_net.combined_True_Fake(\n",
        "                fake_labels = label_fake_img, fake_images = image_output_fake, \n",
        "                true_labels = mini_batch_label_real , true_images = mini_batch_image_real\n",
        "            )\n",
        "\n",
        "            #relocate the combined images into the processing unit \n",
        "            mini_batch_img_comb = mini_batch_img_comb.to(processing_unit)\n",
        "            predicted_label = dis_net.forward_D(mini_batch_img_comb)\n",
        "\n",
        "            #------------\n",
        "            # BackProp Discriminator \n",
        "            #------------\n",
        "            \n",
        "            #move to processing unit \n",
        "            predicted_label = predicted_label.to(processing_unit)\n",
        "            mini_batch_lab_comb = mini_batch_lab_comb.to(processing_unit)\n",
        "\n",
        "            loss_dis = dis_net.function_loss_D(output_label_dis = predicted_label,true_label = mini_batch_lab_comb)\n",
        "            loss_D_batch.append(loss_dis.item())\n",
        "\n",
        "            avg_loss_D = avg_loss_D + loss_dis\n",
        "            print(\"loss DISCRIMINATOR,\", loss_dis ,avg_loss_D)\n",
        "\n",
        "            optimizer_dis.zero_grad()\n",
        "            loss_dis.backward()\n",
        "            optimizer_dis.step()\n",
        "\n",
        "\n",
        "            #----------\n",
        "            #BackProp Generator\n",
        "            #----------\n",
        "\n",
        "            x_gen_input,_ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size, current_batch_dim = current_batch_dim)\n",
        "            x_gen_input = x_gen_input.to(processing_unit)\n",
        "\n",
        "            image_output_fake,fake_target = gen_net.forward_G(x_gen_input)\n",
        "\n",
        "            predicted_label_generated = dis_net.forward_D(image_output_fake)\n",
        "            fake_target = fake_target.view(-1,1)\n",
        "\n",
        "            #move to processing unit\n",
        "            predicted_label_generated = predicted_label_generated.to(processing_unit)\n",
        "            fake_target = fake_target.to(processing_unit)\n",
        "\n",
        "            loss_gen = gen_net.function_loss_G(output_label_gen = predicted_label_generated, true_label = fake_target)\n",
        "\n",
        "            loss_G_batch.append(loss_gen.item())\n",
        "            avg_loss_G = avg_loss_G + loss_gen \n",
        "\n",
        "            print(\"loss GENERATOR\", loss_gen, avg_loss_G)\n",
        "\n",
        "            optimizer_gen.zero_grad()\n",
        "            loss_gen.backward()\n",
        "            optimizer_gen.step()\n",
        "            \n",
        "\n",
        "            last_batch = last_batch_flag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        avg_loss_G = avg_loss_G/counter_batches\n",
        "        avg_loss_D = avg_loss_D/counter_batches\n",
        "\n",
        "        if avg_loss_D < best_loss_D:\n",
        "            best_loss_D = avg_loss_D\n",
        "            best_epoch_D = e + 1\n",
        "            dis_net.save(\"weights_discriminator.pth\")\n",
        "            \n",
        "        if avg_loss_G < best_loss_G:\n",
        "          best_loss_G = avg_loss_G\n",
        "          best_epoch_G = e + 1\n",
        "          gen_net.save(\"weights_generator.pth\")\n",
        "\n",
        "       \n",
        "        print(\"epoch:{0}/{1}\".format(e+1,n_epochs)) \n",
        "              \n",
        "        print((\"DISCRIMIATOR: BCE LOSS={0:.4f}\"\n",
        "                   + (\", BEST!\" if avg_loss_D == best_loss_D else \"\"))\n",
        "                  .format(avg_loss_D))\n",
        "            \n",
        "        print((\"GENERATOR: BCE LOSS={0:.4f}\"\n",
        "                   + (\", BEST!\" if avg_loss_G == best_loss_G else \"\"))\n",
        "                  .format(avg_loss_G))\n",
        "            \n",
        "        print(\"========================================================================================= \\n\")\n",
        "\n",
        "    plt.plot(loss_G_batch)\n",
        "    plt.plot(loss_D_batch)\n",
        "    plt.legend([\"Generator Loss\",\"Discriminator Loss\"])\n",
        "    plt.xlabel(\"# mini-batchs\")\n",
        "    plt.ylabel(\"avg error on each mini-batch\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_lr_obj(architecture_G,architecture_D,batch_size:int,n_epochs:int,dataset,lr_g:int,lr_d:int,processing_unit,p_subset:int):\n",
        "\n",
        "    \"\"\"\n",
        "    Here I modify the learning rate and also the cost function of the Generator. Indeed, now I do not maximize sum(-(1-y)log(D(G))) but rather\n",
        "    I aim to minizime sum(ylog(D(G))). For this reason i falsly impose y = 1. \n",
        "\n",
        "    ----- INPUT -----\n",
        "    Generator -> class of the generator\n",
        "    Discriminator -> class of the discriminator\n",
        "    batch_size,n_epochs,lr -> number\n",
        "    dataset -> -> object dataset  \n",
        "\n",
        "    ----- OUTPUT -----\n",
        "    metrics to measure the accuracy of the models \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    gen_net = architecture_G\n",
        "    dis_net = architecture_D\n",
        "    best_epoch_G = -1\n",
        "    best_epoch_D = -1\n",
        "\n",
        "    dataset.preprocessing(p_subset) \n",
        "    \n",
        "\n",
        "    \n",
        "    optimizer_gen = torch.optim.Adam([p for p in gen_net.net.parameters() if p.requires_grad], lr_g, maximize=False) \n",
        "    optimizer_dis = torch.optim.Adam([p for p in dis_net.net.parameters() if p.requires_grad], lr_d, maximize=False)\n",
        "\n",
        "    best_loss_D = 1000000000000 \n",
        "    best_loss_G = 1000000000000\n",
        "\n",
        "    gen_net.net.train()\n",
        "    dis_net.net.train()\n",
        "\n",
        "    loss_G_batch = []\n",
        "    loss_D_batch = []\n",
        "\n",
        "    for e in range(0,n_epochs):\n",
        "\n",
        "\n",
        "        last_batch = False\n",
        "\n",
        "        image_file, _ = dataset.shuffled_dataset()\n",
        "        length_dataset = len(image_file)\n",
        "\n",
        "        counter_batches = 0\n",
        "        avg_loss_D = 0\n",
        "        avg_loss_G = 0\n",
        "\n",
        "        while not last_batch:\n",
        "\n",
        "            counter_batches = counter_batches + 1\n",
        "\n",
        "            #fake batch creation\n",
        "            x_gen_input, _ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size)\n",
        "            current_batch_dim = x_gen_input.shape\n",
        "\n",
        "            with torch.no_grad():\n",
        "                #move the data into the processing unit device\n",
        "                x_gen_input = x_gen_input.to(processing_unit)\n",
        "                #fake image generated by Generator\n",
        "                image_output_fake,label_fake_img = gen_net.forward_G(x_gen_input)\n",
        "\n",
        "            #true image generation\n",
        "            mini_batch_image_real, mini_batch_label_real, last_batch_flag = dataset.mini_batch_creation(batch_size,image_file)\n",
        "\n",
        "            #combination real/fake image into the cpu\n",
        "            image_output_fake = image_output_fake.to('cpu')\n",
        "            label_fake_img = label_fake_img.to('cpu')\n",
        "            mini_batch_img_comb, mini_batch_lab_comb  = dis_net.combined_True_Fake(\n",
        "                fake_labels = label_fake_img, fake_images = image_output_fake, \n",
        "                true_labels = mini_batch_label_real , true_images = mini_batch_image_real\n",
        "            )\n",
        "\n",
        "            #relocate the combined images into the processing unit \n",
        "            mini_batch_img_comb = mini_batch_img_comb.to(processing_unit)\n",
        "            predicted_label = dis_net.forward_D(mini_batch_img_comb)\n",
        "\n",
        "            #------------\n",
        "            # BackProp Discriminator \n",
        "            #------------\n",
        "            \n",
        "            #move to processing unit \n",
        "            predicted_label = predicted_label.to(processing_unit)\n",
        "            mini_batch_lab_comb = mini_batch_lab_comb.to(processing_unit)\n",
        "\n",
        "            loss_dis = dis_net.function_loss_D(output_label_dis = predicted_label,true_label = mini_batch_lab_comb)\n",
        "            \n",
        "            loss_D_batch.append(loss_dis.item())\n",
        "\n",
        "            avg_loss_D = avg_loss_D + loss_dis\n",
        "            print(\"loss DISCRIMINATOR,\", loss_dis ,avg_loss_D)\n",
        "\n",
        "            optimizer_dis.zero_grad()\n",
        "            loss_dis.backward()\n",
        "            optimizer_dis.step()\n",
        "\n",
        "\n",
        "            #----------\n",
        "            #BackProp Generator\n",
        "            #----------\n",
        "\n",
        "            x_gen_input,_ = gen_net.input_creation(length_dataset = length_dataset , batch_size = batch_size, current_batch_dim = current_batch_dim)\n",
        "            x_gen_input = x_gen_input.to(processing_unit)\n",
        "\n",
        "            image_output_fake,_ = gen_net.forward_G(x_gen_input)\n",
        "            \n",
        "            predicted_label_generated = dis_net.forward_D(image_output_fake)\n",
        "\n",
        "            #move to processing unit\n",
        "            predicted_label_generated = predicted_label_generated.to(processing_unit)\n",
        "            mini_batch_label_real = mini_batch_label_real.view(-1,1).to(processing_unit)#mini_batch_label_real is a tensor full of 1s\n",
        "\n",
        "            loss_gen = gen_net.function_loss_G(output_label_gen=predicted_label_generated, true_label=mini_batch_label_real)    \n",
        "\n",
        "            loss_G_batch.append(loss_gen.item())\n",
        "            avg_loss_G = avg_loss_G + loss_gen \n",
        "\n",
        "            print(\"loss GENERATOR\", loss_gen, avg_loss_G)\n",
        "\n",
        "\n",
        "            optimizer_gen.zero_grad()\n",
        "            loss_gen.backward()\n",
        "            optimizer_gen.step()\n",
        "            \n",
        "\n",
        "            last_batch = last_batch_flag\n",
        "\n",
        "\n",
        "\n",
        "        avg_loss_G = avg_loss_G/counter_batches\n",
        "        avg_loss_D = avg_loss_D/counter_batches\n",
        "\n",
        "        if avg_loss_D < best_loss_D:\n",
        "            best_loss_D = avg_loss_D\n",
        "            best_epoch_D = e + 1\n",
        "            dis_net.save(\"weights_discriminator.pth\")\n",
        "            \n",
        "        if avg_loss_G < best_loss_G:\n",
        "          best_loss_G = avg_loss_G\n",
        "          best_epoch_G = e + 1\n",
        "          gen_net.save(\"weights_generator.pth\")\n",
        "\n",
        "       \n",
        "        print(\"epoch:{0}/{1}\".format(e+1,n_epochs)) \n",
        "              \n",
        "        print((\"DISCRIMIATOR: BCE LOSS={0:.4f}\"\n",
        "                   + (\", BEST!\" if avg_loss_D == best_loss_D else \"\"))\n",
        "                  .format(avg_loss_D))\n",
        "            \n",
        "        print((\"GENERATOR: BCE LOSS={0:.4f}\"\n",
        "                   + (\", BEST!\" if avg_loss_G == best_loss_G else \"\"))\n",
        "                  .format(avg_loss_G))\n",
        "            \n",
        "        print(\"========================================================================================= \\n\")\n",
        "\n",
        "    plt.plot(loss_G_batch)\n",
        "    plt.plot(loss_D_batch)\n",
        "    plt.legend([\"Generator Loss\",\"Discriminator Loss\"])\n",
        "    plt.xlabel(\"# mini-batchs\")\n",
        "    plt.ylabel(\"avg error on each mini-batch\")\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV91YkH8APoi"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMoiOcFqAQs9"
      },
      "outputs": [],
      "source": [
        "processing_unit = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "p_subset = 0.25\n",
        "lr_g = 0.001\n",
        "lr_d = 0.00001\n",
        "dataset = Dataset(path_destination=\"/content/img_celeba_dataset\")\n",
        "gen_net = Generator(processing_unit,\"conventional\")\n",
        "dis_net = Discriminator(processing_unit)\n",
        "\n",
        "train_lr(architecture_G=gen_net,architecture_D=dis_net,batch_size=batch_size,n_epochs=epochs,dataset=dataset,lr_g=lr_g,lr_d=lr_d,processing_unit = processing_unit,p_subset=p_subset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "aKlOURAojHei",
        "outputId": "f8d3095b-4b39-49e0-ef7c-7d0dd277e7c5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FF8BC315C10>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_net.image_generation()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UBv2cPtN-Gr6",
        "HWR8ZEEB-yHw"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "NN_proj_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53) \n[Clang 6.0 (clang-600.0.57)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "8da97b3eb1026c5cd863651ec7b5e28ca1695a5a9d6bc8af194926f5a1e740b7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
